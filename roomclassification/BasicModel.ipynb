{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook contains the basemodel prepared on the dataset to explore the data and come with a base model for understanding. I have\n",
    "performed below tasks:\n",
    "    \n",
    "    1) Data Exploration\n",
    "    2) Data Pre-processing (handling \"-1\" and null values)\n",
    "    3) Normalizing the feature values using StandardScalar functions in scikit-learn\n",
    "    4) Performing feature selection using random forest classifier feature importance\n",
    "    5) Training the dataset over basic state of the art classification models\n",
    "    6) Interpreting baseline model results and using them for the final model for the better results\n",
    "    \n",
    " Intution: I am starting this problem as a binary classification just to understand the data, but, I sense that our job is to just identify \"toilet\" I may have to handle it as an outlier detection problem (like using OneClassSVM() that uses concept of cost sensetive learning). The more research will be done while making the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration\n",
    "   \n",
    "    1) Loading the dataset using pandas csv command\n",
    "    2) Checking out the features and samples of the data (shape)\n",
    "    3) Checking out the first few records\n",
    "    4) As I am dealing with supervised classification problem, the label columns which is at the first column of the dataset has to be verified. We are supposed to classify the labels as \"toilet\" and \"not-toilet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv('ml_problem_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3163, 218)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding the shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>house_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spigot</td>\n",
       "      <td>house_126</td>\n",
       "      <td>13.007144</td>\n",
       "      <td>12.230354</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6.197</td>\n",
       "      <td>6.484</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>8.488526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591493</td>\n",
       "      <td>0.705452</td>\n",
       "      <td>0.633203</td>\n",
       "      <td>0.443873</td>\n",
       "      <td>0.406736</td>\n",
       "      <td>0.451285</td>\n",
       "      <td>0.411402</td>\n",
       "      <td>0.282912</td>\n",
       "      <td>0.285625</td>\n",
       "      <td>0.306219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spigot</td>\n",
       "      <td>house_126</td>\n",
       "      <td>9.230357</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.176</td>\n",
       "      <td>10.695</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427394</td>\n",
       "      <td>0.689973</td>\n",
       "      <td>0.706920</td>\n",
       "      <td>0.352133</td>\n",
       "      <td>0.313228</td>\n",
       "      <td>0.506318</td>\n",
       "      <td>0.464877</td>\n",
       "      <td>0.194555</td>\n",
       "      <td>0.193622</td>\n",
       "      <td>0.339895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spigot</td>\n",
       "      <td>house_126</td>\n",
       "      <td>9.258934</td>\n",
       "      <td>10.839287</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.978</td>\n",
       "      <td>7.999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.842413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547300</td>\n",
       "      <td>0.732130</td>\n",
       "      <td>0.652021</td>\n",
       "      <td>0.409212</td>\n",
       "      <td>0.394238</td>\n",
       "      <td>0.493608</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.264117</td>\n",
       "      <td>0.247763</td>\n",
       "      <td>0.298353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>laundrytub</td>\n",
       "      <td>house_25</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.764</td>\n",
       "      <td>3.699</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>1.467188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554086</td>\n",
       "      <td>0.273246</td>\n",
       "      <td>0.811351</td>\n",
       "      <td>0.014543</td>\n",
       "      <td>-0.430410</td>\n",
       "      <td>0.034226</td>\n",
       "      <td>0.181226</td>\n",
       "      <td>0.293240</td>\n",
       "      <td>0.273289</td>\n",
       "      <td>0.195252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>laundrytub</td>\n",
       "      <td>house_25</td>\n",
       "      <td>1.887500</td>\n",
       "      <td>2.776779</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.084</td>\n",
       "      <td>4.764</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.607542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595937</td>\n",
       "      <td>0.410507</td>\n",
       "      <td>0.923314</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>-0.036309</td>\n",
       "      <td>0.433407</td>\n",
       "      <td>0.424058</td>\n",
       "      <td>0.584138</td>\n",
       "      <td>0.445210</td>\n",
       "      <td>0.307080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shower</td>\n",
       "      <td>house_64</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.158929</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.026</td>\n",
       "      <td>7.992</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.392351</td>\n",
       "      <td>0.634925</td>\n",
       "      <td>1.486128</td>\n",
       "      <td>0.547370</td>\n",
       "      <td>0.938872</td>\n",
       "      <td>1.617265</td>\n",
       "      <td>1.297368</td>\n",
       "      <td>0.280025</td>\n",
       "      <td>0.273351</td>\n",
       "      <td>0.361752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shower</td>\n",
       "      <td>house_64</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.150</td>\n",
       "      <td>3.006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496755</td>\n",
       "      <td>1.244954</td>\n",
       "      <td>1.179685</td>\n",
       "      <td>-0.057512</td>\n",
       "      <td>-0.023149</td>\n",
       "      <td>0.862283</td>\n",
       "      <td>0.792202</td>\n",
       "      <td>-0.158902</td>\n",
       "      <td>0.081314</td>\n",
       "      <td>0.336621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shower</td>\n",
       "      <td>house_64</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.006</td>\n",
       "      <td>3.028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472997</td>\n",
       "      <td>0.939928</td>\n",
       "      <td>1.498492</td>\n",
       "      <td>-0.307549</td>\n",
       "      <td>0.167246</td>\n",
       "      <td>0.875433</td>\n",
       "      <td>0.782044</td>\n",
       "      <td>0.100265</td>\n",
       "      <td>0.173034</td>\n",
       "      <td>0.393343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shower</td>\n",
       "      <td>house_64</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.067</td>\n",
       "      <td>3.007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247817</td>\n",
       "      <td>1.210548</td>\n",
       "      <td>1.263601</td>\n",
       "      <td>-0.131124</td>\n",
       "      <td>0.036350</td>\n",
       "      <td>0.903258</td>\n",
       "      <td>0.630470</td>\n",
       "      <td>-0.074563</td>\n",
       "      <td>0.123094</td>\n",
       "      <td>0.514758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shower</td>\n",
       "      <td>house_64</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.088</td>\n",
       "      <td>3.007</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304559</td>\n",
       "      <td>1.228364</td>\n",
       "      <td>1.316550</td>\n",
       "      <td>-0.063319</td>\n",
       "      <td>-0.040352</td>\n",
       "      <td>0.869791</td>\n",
       "      <td>0.681914</td>\n",
       "      <td>-0.041953</td>\n",
       "      <td>0.169518</td>\n",
       "      <td>0.550178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label   house_id  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0      spigot  house_126  13.007144  12.230354          4          5   \n",
       "1      spigot  house_126   9.230357  -1.000000         -1          1   \n",
       "2      spigot  house_126   9.258934  10.839287          1          3   \n",
       "3  laundrytub   house_25   1.687500   2.562500          2          3   \n",
       "4  laundrytub   house_25   1.887500   2.776779          3          1   \n",
       "5      shower   house_64  -1.000000   5.158929         -1         -1   \n",
       "6      shower   house_64  -1.000000  -1.000000         -1         -1   \n",
       "7      shower   house_64  -1.000000  -1.000000         -1         -1   \n",
       "8      shower   house_64  -1.000000  -1.000000         -1         -1   \n",
       "9      shower   house_64  -1.000000  -1.000000         -1         -1   \n",
       "\n",
       "   feature_5  feature_6  feature_7  feature_8     ...       feature_207  \\\n",
       "0      6.197      6.484   0.977778   8.488526     ...          0.591493   \n",
       "1     14.176     10.695   0.928571   0.000000     ...          0.427394   \n",
       "2      8.978      7.999   1.000000   6.842413     ...          0.547300   \n",
       "3      4.764      3.699   0.941176   1.467188     ...          0.554086   \n",
       "4      3.084      4.764   0.944444   1.607542     ...          0.595937   \n",
       "5      3.026      7.992   0.914286   0.000000     ...         -1.392351   \n",
       "6      3.150      3.006   1.000000   0.000000     ...         -0.496755   \n",
       "7      3.006      3.028   1.000000   0.000000     ...         -0.472997   \n",
       "8      3.067      3.007   1.000000   0.000000     ...         -0.247817   \n",
       "9      3.088      3.007   0.967742   0.000000     ...         -0.304559   \n",
       "\n",
       "   feature_208  feature_209  feature_210  feature_211  feature_212  \\\n",
       "0     0.705452     0.633203     0.443873     0.406736     0.451285   \n",
       "1     0.689973     0.706920     0.352133     0.313228     0.506318   \n",
       "2     0.732130     0.652021     0.409212     0.394238     0.493608   \n",
       "3     0.273246     0.811351     0.014543    -0.430410     0.034226   \n",
       "4     0.410507     0.923314     0.269123    -0.036309     0.433407   \n",
       "5     0.634925     1.486128     0.547370     0.938872     1.617265   \n",
       "6     1.244954     1.179685    -0.057512    -0.023149     0.862283   \n",
       "7     0.939928     1.498492    -0.307549     0.167246     0.875433   \n",
       "8     1.210548     1.263601    -0.131124     0.036350     0.903258   \n",
       "9     1.228364     1.316550    -0.063319    -0.040352     0.869791   \n",
       "\n",
       "   feature_213  feature_214  feature_215  feature_216  \n",
       "0     0.411402     0.282912     0.285625     0.306219  \n",
       "1     0.464877     0.194555     0.193622     0.339895  \n",
       "2     0.407968     0.264117     0.247763     0.298353  \n",
       "3     0.181226     0.293240     0.273289     0.195252  \n",
       "4     0.424058     0.584138     0.445210     0.307080  \n",
       "5     1.297368     0.280025     0.273351     0.361752  \n",
       "6     0.792202    -0.158902     0.081314     0.336621  \n",
       "7     0.782044     0.100265     0.173034     0.393343  \n",
       "8     0.630470    -0.074563     0.123094     0.514758  \n",
       "9     0.681914    -0.041953     0.169518     0.550178  \n",
       "\n",
       "[10 rows x 218 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out the first few samples\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the above samples, only the \"house_id\" and \"label\" are categorial and rest are continous\n",
    "\n",
    "As this is basemodel, dropping the \"house_id\" column it seemes irrelevant for the \"label\" classification\n",
    "\n",
    "There are missing values loaded as \"-1\". As of now replacing them with \"NaN\" values and dropping them\n",
    "\n",
    "Next, I will be encoding the categorial \"label\" into numerical (though there are various technqiues, as of now I am encoding them manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink          1361\n",
       "shower         741\n",
       "bathtub        453\n",
       "toilet         382\n",
       "laundrytub     176\n",
       "spigot          50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At first, we are seeing how many samples are there for each \"label\" type\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the missing values i.e. \"-1\" with \"NaN\" values\n",
    "data = data.replace(-1,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1358, 218)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping rows containing null values - as of now we are dropping, will be analyzed further in the final model\n",
    "data = data.dropna()\n",
    "#checking out the new shape- how many rows lefft\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my task is to just classify the data which has \"toilet\" and \"not a toilet\", I am handling it as a binary classification problem, Therefore, I am encoding all \"toilet\" labels as \"0\" and rest of the labels as \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorial labels\n",
    "data['label'].replace({'toilet':0,'sink':1,'shower':1,'bathtub':1,'laundrytub':1,'spigot':1},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>house_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>house_126</td>\n",
       "      <td>13.007144</td>\n",
       "      <td>12.230354</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.197</td>\n",
       "      <td>6.484</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>8.488526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591493</td>\n",
       "      <td>0.705452</td>\n",
       "      <td>0.633203</td>\n",
       "      <td>0.443873</td>\n",
       "      <td>0.406736</td>\n",
       "      <td>0.451285</td>\n",
       "      <td>0.411402</td>\n",
       "      <td>0.282912</td>\n",
       "      <td>0.285625</td>\n",
       "      <td>0.306219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>house_126</td>\n",
       "      <td>9.258934</td>\n",
       "      <td>10.839287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.978</td>\n",
       "      <td>7.999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.842413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547300</td>\n",
       "      <td>0.732130</td>\n",
       "      <td>0.652021</td>\n",
       "      <td>0.409212</td>\n",
       "      <td>0.394238</td>\n",
       "      <td>0.493608</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.264117</td>\n",
       "      <td>0.247763</td>\n",
       "      <td>0.298353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>house_25</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.764</td>\n",
       "      <td>3.699</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>1.467188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554086</td>\n",
       "      <td>0.273246</td>\n",
       "      <td>0.811351</td>\n",
       "      <td>0.014543</td>\n",
       "      <td>-0.430410</td>\n",
       "      <td>0.034226</td>\n",
       "      <td>0.181226</td>\n",
       "      <td>0.293240</td>\n",
       "      <td>0.273289</td>\n",
       "      <td>0.195252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>house_25</td>\n",
       "      <td>1.887500</td>\n",
       "      <td>2.776779</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.084</td>\n",
       "      <td>4.764</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.607542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595937</td>\n",
       "      <td>0.410507</td>\n",
       "      <td>0.923314</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>-0.036309</td>\n",
       "      <td>0.433407</td>\n",
       "      <td>0.424058</td>\n",
       "      <td>0.584138</td>\n",
       "      <td>0.445210</td>\n",
       "      <td>0.307080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>house_5</td>\n",
       "      <td>0.598215</td>\n",
       "      <td>0.596429</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.264</td>\n",
       "      <td>5.552</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.403125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170150</td>\n",
       "      <td>-0.977168</td>\n",
       "      <td>-1.611767</td>\n",
       "      <td>-1.201917</td>\n",
       "      <td>-0.921476</td>\n",
       "      <td>-0.819959</td>\n",
       "      <td>-0.419842</td>\n",
       "      <td>-0.203723</td>\n",
       "      <td>0.200655</td>\n",
       "      <td>0.781734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label   house_id  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0       1  house_126  13.007144  12.230354        4.0        5.0      6.197   \n",
       "2       1  house_126   9.258934  10.839287        1.0        3.0      8.978   \n",
       "3       1   house_25   1.687500   2.562500        2.0        3.0      4.764   \n",
       "4       1   house_25   1.887500   2.776779        3.0        1.0      3.084   \n",
       "20      1    house_5   0.598215   0.596429       21.0       21.0      5.264   \n",
       "\n",
       "    feature_6  feature_7  feature_8     ...       feature_207  feature_208  \\\n",
       "0       6.484   0.977778   8.488526     ...          0.591493     0.705452   \n",
       "2       7.999   1.000000   6.842413     ...          0.547300     0.732130   \n",
       "3       3.699   0.941176   1.467188     ...          0.554086     0.273246   \n",
       "4       4.764   0.944444   1.607542     ...          0.595937     0.410507   \n",
       "20      5.552   0.926829   0.403125     ...         -0.170150    -0.977168   \n",
       "\n",
       "    feature_209  feature_210  feature_211  feature_212  feature_213  \\\n",
       "0      0.633203     0.443873     0.406736     0.451285     0.411402   \n",
       "2      0.652021     0.409212     0.394238     0.493608     0.407968   \n",
       "3      0.811351     0.014543    -0.430410     0.034226     0.181226   \n",
       "4      0.923314     0.269123    -0.036309     0.433407     0.424058   \n",
       "20    -1.611767    -1.201917    -0.921476    -0.819959    -0.419842   \n",
       "\n",
       "    feature_214  feature_215  feature_216  \n",
       "0      0.282912     0.285625     0.306219  \n",
       "2      0.264117     0.247763     0.298353  \n",
       "3      0.293240     0.273289     0.195252  \n",
       "4      0.584138     0.445210     0.307080  \n",
       "20    -0.203723     0.200655     0.781734  \n",
       "\n",
       "[5 rows x 218 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out again the first few records to make sure that changes are observed\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1172\n",
       "0     186\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking again the number of samples of each class \"Class 0 - toilet\" and \"Class 1 - \"Not a Toilet\"\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the \"house_id\" column - seems unimportant as of now\n",
    "data.drop(['house_id'],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1358, 217)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the shape- we can observe that one column is decrease as we dropped the \"house_id\" column\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating the data as features and labels\n",
    "original_features=data.iloc[:,1:]\n",
    "labels = data.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - Feature Normalization and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Scikit learn libraries\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#Normalizing Train Data Features\n",
    "scaler_features = scaler.fit(original_features)\n",
    "normalized_features = scaler_features.transform(semi_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273564</td>\n",
       "      <td>0.337442</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.241643</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.961254</td>\n",
       "      <td>0.377223</td>\n",
       "      <td>0.348271</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603409</td>\n",
       "      <td>0.444102</td>\n",
       "      <td>0.502134</td>\n",
       "      <td>0.558852</td>\n",
       "      <td>0.501970</td>\n",
       "      <td>0.560687</td>\n",
       "      <td>0.396166</td>\n",
       "      <td>0.413017</td>\n",
       "      <td>0.546973</td>\n",
       "      <td>0.590583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194110</td>\n",
       "      <td>0.298723</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.451974</td>\n",
       "      <td>0.302949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.304071</td>\n",
       "      <td>0.276357</td>\n",
       "      <td>0.178332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594784</td>\n",
       "      <td>0.449806</td>\n",
       "      <td>0.506343</td>\n",
       "      <td>0.552675</td>\n",
       "      <td>0.499769</td>\n",
       "      <td>0.570761</td>\n",
       "      <td>0.395281</td>\n",
       "      <td>0.407852</td>\n",
       "      <td>0.531820</td>\n",
       "      <td>0.587554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.068343</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.133263</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.065201</td>\n",
       "      <td>0.130835</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596108</td>\n",
       "      <td>0.351691</td>\n",
       "      <td>0.541980</td>\n",
       "      <td>0.482339</td>\n",
       "      <td>0.354526</td>\n",
       "      <td>0.461410</td>\n",
       "      <td>0.336896</td>\n",
       "      <td>0.415855</td>\n",
       "      <td>0.542036</td>\n",
       "      <td>0.547846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.074308</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>0.106627</td>\n",
       "      <td>0.903134</td>\n",
       "      <td>0.071438</td>\n",
       "      <td>0.134411</td>\n",
       "      <td>0.069869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604276</td>\n",
       "      <td>0.381039</td>\n",
       "      <td>0.567023</td>\n",
       "      <td>0.527709</td>\n",
       "      <td>0.423938</td>\n",
       "      <td>0.556431</td>\n",
       "      <td>0.399425</td>\n",
       "      <td>0.495792</td>\n",
       "      <td>0.610841</td>\n",
       "      <td>0.590915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010523</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.171078</td>\n",
       "      <td>0.154448</td>\n",
       "      <td>0.872420</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454755</td>\n",
       "      <td>0.084339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265549</td>\n",
       "      <td>0.268036</td>\n",
       "      <td>0.258080</td>\n",
       "      <td>0.182121</td>\n",
       "      <td>0.279293</td>\n",
       "      <td>0.512967</td>\n",
       "      <td>0.773721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.273564  0.337442  0.081633  0.089286  0.241643  0.211009  0.961254   \n",
       "1  0.194110  0.298723  0.020408  0.053571  0.451974  0.302949  1.000000   \n",
       "2  0.033613  0.068343  0.040816  0.053571  0.133263  0.041995  0.897436   \n",
       "3  0.037853  0.074308  0.061224  0.017857  0.006202  0.106627  0.903134   \n",
       "4  0.010523  0.013619  0.428571  0.375000  0.171078  0.154448  0.872420   \n",
       "\n",
       "        7         8         9      ...          206       207       208  \\\n",
       "0  0.377223  0.348271  0.143219    ...     0.603409  0.444102  0.502134   \n",
       "1  0.304071  0.276357  0.178332    ...     0.594784  0.449806  0.506343   \n",
       "2  0.065201  0.130835  0.137757    ...     0.596108  0.351691  0.541980   \n",
       "3  0.071438  0.134411  0.069869    ...     0.604276  0.381039  0.567023   \n",
       "4  0.017915  0.110400  0.096400    ...     0.454755  0.084339  0.000000   \n",
       "\n",
       "        209       210       211       212       213       214       215  \n",
       "0  0.558852  0.501970  0.560687  0.396166  0.413017  0.546973  0.590583  \n",
       "1  0.552675  0.499769  0.570761  0.395281  0.407852  0.531820  0.587554  \n",
       "2  0.482339  0.354526  0.461410  0.336896  0.415855  0.542036  0.547846  \n",
       "3  0.527709  0.423938  0.556431  0.399425  0.495792  0.610841  0.590915  \n",
       "4  0.265549  0.268036  0.258080  0.182121  0.279293  0.512967  0.773721  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking out the whether the features are normalzied - Observe Column 6 in the second cell and here to find difference\n",
    "pd.DataFrame(normalized_features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost 216 features. All of them are unknown to me in terms of domain knowledge. Hence, I am using all the features for the feature selection using RandomForesTrees.\n",
    "RandomForestTress is the state of the art techniques for the feature selection. It ranks all the features in the order of their importances adding to classify the label\n",
    "One can select the top \"K\" important features that are required for the classification model\n",
    "\n",
    "Reference: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACOcAAAGrCAYAAACB2mHGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19492d02668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Matplotlib for plotting the feature importance graph\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def FeatureSelection(myinputX, myinputY):\n",
    "\n",
    "    labels = np.array(myinputY).astype(int).ravel()\n",
    "    inputX = np.array(myinputX)\n",
    "    \n",
    "    #Random Forest Model\n",
    "    model = RandomForestClassifier(random_state = 0)\n",
    "    model.fit(inputX,labels)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    \n",
    "    #Plotting the Features agains their importance scores\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "    plt.figure(figsize = (40,7))\n",
    "    plt.title(\"Feature importances (y-axis) vs Features IDs(x-axis)\")\n",
    "    plt.bar(range(inputX.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(inputX.shape[1]), indices)\n",
    "    plt.xlim([-1, inputX.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "    # Selecting top featueres which have higher importance values = here we can find \"80\" features\n",
    "    # I am changing number of features manually as of now. One has to find the tradeoff\n",
    "    \n",
    "    \n",
    "    myinputX = pd.DataFrame(myinputX)\n",
    "    newX = myinputX.iloc[:,model.feature_importances_.argsort()[::-1][:80]]\n",
    "    \n",
    "    # Converting the X dataframe into tensors and returning the reduced feature set\n",
    "    myX = newX.as_matrix()\n",
    "\n",
    "    return myX\n",
    "\n",
    "# Reduced Feature Set\n",
    "reduced_features  = FeatureSelection(normalized_features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graphs displays the \"plot\" of the importance of each feature with respect to the label. The selection criteria can be made using a recurisve algorithm that removes the reduandant and irrelevant features. As of now I am selecting top \"80\" features. And the below cell is to verify whether the features are reduced to \"80\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.191710</td>\n",
       "      <td>0.288241</td>\n",
       "      <td>0.242407</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>0.696945</td>\n",
       "      <td>0.559075</td>\n",
       "      <td>0.643178</td>\n",
       "      <td>0.436906</td>\n",
       "      <td>0.743709</td>\n",
       "      <td>0.952264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474513</td>\n",
       "      <td>0.400314</td>\n",
       "      <td>0.166867</td>\n",
       "      <td>0.106966</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.244954</td>\n",
       "      <td>0.138743</td>\n",
       "      <td>0.546973</td>\n",
       "      <td>0.181652</td>\n",
       "      <td>0.121444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196891</td>\n",
       "      <td>0.227297</td>\n",
       "      <td>0.205501</td>\n",
       "      <td>0.065719</td>\n",
       "      <td>0.621295</td>\n",
       "      <td>0.500836</td>\n",
       "      <td>0.553311</td>\n",
       "      <td>0.400956</td>\n",
       "      <td>0.705988</td>\n",
       "      <td>0.896717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450568</td>\n",
       "      <td>0.534241</td>\n",
       "      <td>0.187478</td>\n",
       "      <td>0.021968</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.285403</td>\n",
       "      <td>0.034776</td>\n",
       "      <td>0.531820</td>\n",
       "      <td>0.417778</td>\n",
       "      <td>0.006942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134737</td>\n",
       "      <td>0.086778</td>\n",
       "      <td>0.099085</td>\n",
       "      <td>0.076091</td>\n",
       "      <td>0.662657</td>\n",
       "      <td>0.545788</td>\n",
       "      <td>0.546692</td>\n",
       "      <td>0.205553</td>\n",
       "      <td>0.812276</td>\n",
       "      <td>0.944147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421828</td>\n",
       "      <td>0.359710</td>\n",
       "      <td>0.222237</td>\n",
       "      <td>0.028138</td>\n",
       "      <td>0.031813</td>\n",
       "      <td>0.050687</td>\n",
       "      <td>0.031508</td>\n",
       "      <td>0.542036</td>\n",
       "      <td>0.540328</td>\n",
       "      <td>0.030361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139896</td>\n",
       "      <td>0.089709</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.017724</td>\n",
       "      <td>0.696272</td>\n",
       "      <td>0.565252</td>\n",
       "      <td>0.555887</td>\n",
       "      <td>0.211052</td>\n",
       "      <td>0.763075</td>\n",
       "      <td>0.961126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608810</td>\n",
       "      <td>0.385490</td>\n",
       "      <td>0.250406</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.046858</td>\n",
       "      <td>0.610841</td>\n",
       "      <td>0.482903</td>\n",
       "      <td>0.036650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160622</td>\n",
       "      <td>0.076166</td>\n",
       "      <td>0.067841</td>\n",
       "      <td>0.159022</td>\n",
       "      <td>0.295233</td>\n",
       "      <td>0.553244</td>\n",
       "      <td>0.273694</td>\n",
       "      <td>0.157908</td>\n",
       "      <td>0.713775</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592255</td>\n",
       "      <td>0.515539</td>\n",
       "      <td>0.100719</td>\n",
       "      <td>0.047757</td>\n",
       "      <td>0.149613</td>\n",
       "      <td>0.029822</td>\n",
       "      <td>0.042244</td>\n",
       "      <td>0.512967</td>\n",
       "      <td>0.209132</td>\n",
       "      <td>0.074912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.191710  0.288241  0.242407  0.058908  0.696945  0.559075  0.643178   \n",
       "1  0.196891  0.227297  0.205501  0.065719  0.621295  0.500836  0.553311   \n",
       "2  0.134737  0.086778  0.099085  0.076091  0.662657  0.545788  0.546692   \n",
       "3  0.139896  0.089709  0.097561  0.017724  0.696272  0.565252  0.555887   \n",
       "4  0.160622  0.076166  0.067841  0.159022  0.295233  0.553244  0.273694   \n",
       "\n",
       "         7         8         9     ...           70        71        72  \\\n",
       "0  0.436906  0.743709  0.952264    ...     0.474513  0.400314  0.166867   \n",
       "1  0.400956  0.705988  0.896717    ...     0.450568  0.534241  0.187478   \n",
       "2  0.205553  0.812276  0.944147    ...     0.421828  0.359710  0.222237   \n",
       "3  0.211052  0.763075  0.961126    ...     0.608810  0.385490  0.250406   \n",
       "4  0.157908  0.713775  0.936753    ...     0.592255  0.515539  0.100719   \n",
       "\n",
       "         73        74        75        76        77        78        79  \n",
       "0  0.106966  0.025561  0.244954  0.138743  0.546973  0.181652  0.121444  \n",
       "1  0.021968  0.013650  0.285403  0.034776  0.531820  0.417778  0.006942  \n",
       "2  0.028138  0.031813  0.050687  0.031508  0.542036  0.540328  0.030361  \n",
       "3  0.029847  0.023197  0.024088  0.046858  0.610841  0.482903  0.036650  \n",
       "4  0.047757  0.149613  0.029822  0.042244  0.512967  0.209132  0.074912  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reduced_features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models - Model Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have performed below steps for model training and testing\n",
    "\n",
    "  1) Using train_test_split - splitting the dataset into 85:15 ratio\n",
    "  \n",
    "  2) I made sure there are \"toilet\" samples in both training and testing dataset\n",
    "  \n",
    "  3) I have used a python dictionary to take 5 basic classifiers and performed operatio using training and testing set one by one\n",
    "  \n",
    "  4) The classification metrics report and the confusion matrix are used as of now to interpret the high level results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples of each class label in training dataset is:\n",
      "(array([0, 1], dtype=int64), array([167, 987], dtype=int64))\n",
      "Number of samples of each class label in testing dataset is:\n",
      "(array([0, 1], dtype=int64), array([ 19, 185], dtype=int64))\n",
      "Results of the classifier model GB\n",
      "Confusion Matrix for the classifier model GB\n",
      "[[  9  10]\n",
      " [  3 182]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.47      0.58        19\n",
      "          1       0.95      0.98      0.97       185\n",
      "\n",
      "avg / total       0.93      0.94      0.93       204\n",
      "\n",
      "Results of the classifier model LR\n",
      "Confusion Matrix for the classifier model LR\n",
      "[[  0  19]\n",
      " [  0 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        19\n",
      "          1       0.91      1.00      0.95       185\n",
      "\n",
      "avg / total       0.82      0.91      0.86       204\n",
      "\n",
      "Results of the classifier model NB\n",
      "Confusion Matrix for the classifier model NB\n",
      "[[  9  10]\n",
      " [ 55 130]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.14      0.47      0.22        19\n",
      "          1       0.93      0.70      0.80       185\n",
      "\n",
      "avg / total       0.86      0.68      0.75       204\n",
      "\n",
      "Results of the classifier model DT\n",
      "Confusion Matrix for the classifier model DT\n",
      "[[ 13   6]\n",
      " [ 14 171]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.68      0.57        19\n",
      "          1       0.97      0.92      0.94       185\n",
      "\n",
      "avg / total       0.92      0.90      0.91       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manojkumar_meno\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the classifier model SVC\n",
      "Confusion Matrix for the classifier model SVC\n",
      "[[  0  19]\n",
      " [  0 185]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        19\n",
      "          1       0.91      1.00      0.95       185\n",
      "\n",
      "avg / total       0.82      0.91      0.86       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing the sklearn dependencies\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#Train - Test Split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        reduced_features, \n",
    "        labels.as_matrix(),\n",
    "        train_size=0.85, \n",
    "        random_state=1234)\n",
    "\n",
    "#Checking whether both the training and testing set has acceptable number of \"toilet\" samples\n",
    "print (\"Number of samples of each class label in training dataset is:\")\n",
    "print (np.unique(y_train, return_counts=True))\n",
    "print (\"Number of samples of each class label in testing dataset is:\")\n",
    "print (np.unique(y_test, return_counts=True))\n",
    "\n",
    "# Making a dictionary kind of pipeline for few basic classifiers- randomstate to reproduce the results\n",
    "Classifiers =  {\"GB\": {\"f\": GradientBoostingClassifier(random_state = 42)},\n",
    "                \"DT\": {\"f\": DecisionTreeClassifier(random_state = 42)},\n",
    "                \"LR\": {\"f\": LogisticRegression(random_state = 42)},\n",
    "                \"NB\": {\"f\": GaussianNB()},\n",
    "                \"SVC\": {\"f\": SVC(random_state = 42)}}\n",
    "                \n",
    "#Preparing the metrics dictionary that will be used in the evaluation\n",
    "measures = {\"class_report\": metrics.classification_report, \"conf_mat\": metrics.confusion_matrix}\n",
    "\n",
    "\n",
    "#Models Training and prediction - one by one\n",
    "for model in Classifiers.keys():\n",
    "\n",
    "    # Fit the Classifiers using the training dataset\n",
    "    Classifiers[model][\"f\"].fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # Predict\n",
    "    Classifiers[model][\"c\"] = Classifiers[model][\"f\"].predict(X_test)\n",
    "    \n",
    "    # Evaluate each model in Regressors\n",
    "    print (\"Results of the classifier model {}\".format(model))\n",
    "    print (\"Confusion Matrix for the classifier model {}\".format(model))\n",
    "    \n",
    "    #printing the metrics results\n",
    "    for measure in measures.keys():\n",
    "        print (measures[measure](y_test.ravel(), Classifiers[model][\"c\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen that GradientBoost Classifier is performing better when compared to other classifiers.\n",
    "\n",
    "It is able to truly predict 9 samples as \"toilet\" out of \"19\" samples.\n",
    "\n",
    "The fact that this dataset is imbalanced, the precision of \"Toilet\" class is not up to the mark for all the models\n",
    "\n",
    "Basic Plan from here:\n",
    "\n",
    "    1) Use ensemeble techqnieus to handle the imbalanced dataset\n",
    "    2) Use cost sensitive learning such that we give more weight to the \"toilet\" class as it is our priority\n",
    "    3) Tune the algorithms using GridSearchCV\n",
    "    4) Though, neural networks are not required with the size of dataset, we can give a try\n",
    "    5) Use XGBoost\n",
    "    6) Better feature selection\n",
    "    7) Use other classification metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
